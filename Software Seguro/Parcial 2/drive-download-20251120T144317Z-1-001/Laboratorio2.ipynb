{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec75e86c-4887-4ca0-a20a-acd67af04b64",
   "metadata": {},
   "source": [
    "# Predicci√≥n de Vulnerabilidades en Commits de GitHub\n",
    "\n",
    "**Descripci√≥n General** <br>\n",
    "Este laboratorio utiliza t√©cnicas de Machine Learning para analizar commits de GitHub y predecir:\n",
    "- **Clasificaci√≥n Binaria:** Si un c√≥digo es seguro o vulnerable\n",
    "- **Clasificaci√≥n Multiclase:** El tipo espec√≠fico de vulnerabilidad\n",
    "\n",
    "***Para ejecutar el proyecto:***\n",
    "1. Ejecutar celdas en orden: Ejecutar cada celda secuencialmente\n",
    "2. Ver resultados intermedios: Cada celda muestra progreso y resultados\n",
    "3. Modificar par√°metros: Puede ajustar el tama√±o del dataset en Celda 2, funcion ***generate_vulnerability_dataset***\n",
    "4. Probar con nuevos datos: Use la funci√≥n load_and_predict() con sus propios datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ef923-41a5-4276-ba96-1f47e24ba11b",
   "metadata": {},
   "source": [
    "## Importaci√≥n de Librer√≠as\n",
    "Importamos todas las librer√≠as necesarias para el proyecto:\n",
    "- pandas y numpy para manipulaci√≥n de datos\n",
    "- sklearn para machine learning y preprocesamiento\n",
    "- xgboost como algoritmo avanzado\n",
    "- matplotlib y seaborn para visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32eb9ed-c18e-40d1-81b1-73804c01afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n de librer√≠as esenciales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Utilidades\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc7d91-af8f-4191-bd63-8c839bd90ebf",
   "metadata": {},
   "source": [
    "## Generaci√≥n del Dataset Sint√©tico\n",
    "Esta funci√≥n crea un dataset sint√©tico realista que simula commits de GitHub con:\n",
    "- Caracter√≠sticas t√©cnicas (l√≠neas de c√≥digo, complejidad, etc.)\n",
    "- Patrones de seguridad espec√≠ficos por lenguaje\n",
    "- Historial de desarrolladores\n",
    "- Etiquetas de vulnerabilidad basadas en OWASP Top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6353ea-a9ea-4554-86dc-6564adaf37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vulnerability_dataset(n_samples=2000):\n",
    "    \"\"\"\n",
    "    Genera un dataset sint√©tico de commits de GitHub con caracter√≠sticas\n",
    "    de seguridad para clasificaci√≥n de vulnerabilidades\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Dataset con caracter√≠sticas de commits y etiquetas de seguridad\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuraci√≥n reproducible\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Lenguajes de programaci√≥n comunes en GitHub\n",
    "    languages = ['JavaScript', 'Python', 'Java', 'C++', 'C', 'PHP', 'Ruby', 'Go', 'Rust']\n",
    "    \n",
    "    # Tipos de vulnerabilidad basados en OWASP Top 10\n",
    "    vulnerability_types = [\n",
    "        'SQL_Injection', 'XSS', 'Buffer_Overflow', 'Insecure_Authentication',\n",
    "        'Path_Traversal', 'Command_Injection', 'XXE', 'Deserialization',\n",
    "        'Cryptographic_Weakness', 'Secure', 'Secure'  # Doble peso para 'Secure'\n",
    "    ]\n",
    "    \n",
    "    # Palabras clave en mensajes de commit por tipo de vulnerabilidad\n",
    "    vulnerability_keywords = {\n",
    "        'SQL_Injection': ['sql', 'query', 'database', 'escape', 'parameterize', 'orm'],\n",
    "        'XSS': ['html', 'script', 'escape', 'sanitize', 'dom', 'innerhtml'],\n",
    "        'Buffer_Overflow': ['buffer', 'memory', 'alloc', 'size', 'length', 'boundary'],\n",
    "        'Insecure_Authentication': ['auth', 'password', 'token', 'session', 'cookie', 'jwt'],\n",
    "        'Path_Traversal': ['path', 'file', 'directory', 'traverse', 'upload'],\n",
    "        'Command_Injection': ['command', 'exec', 'system', 'shell', 'subprocess'],\n",
    "        'XXE': ['xml', 'external', 'entity', 'parser'],\n",
    "        'Deserialization': ['serialize', 'deserialize', 'json', 'yaml', 'pickle'],\n",
    "        'Cryptographic_Weakness': ['crypto', 'encrypt', 'hash', 'salt', 'iv', 'key'],\n",
    "        'Secure': ['refactor', 'optimize', 'feature', 'documentation', 'test', 'style']\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Caracter√≠sticas b√°sicas del commit\n",
    "        language = random.choice(languages)\n",
    "        vulnerability_type = random.choice(vulnerability_types)\n",
    "        \n",
    "        # Determinar si es vulnerable basado en el tipo\n",
    "        is_vulnerable = 1 if vulnerability_type != 'Secure' else 0\n",
    "        \n",
    "        # Caracter√≠sticas t√©cnicas del commit\n",
    "        lines_added = np.random.poisson(50)  # N√∫mero de l√≠neas agregadas\n",
    "        lines_deleted = np.random.poisson(20)  # N√∫mero de l√≠neas eliminadas\n",
    "        files_changed = np.random.poisson(3)  # Archivos modificados\n",
    "        \n",
    "        # M√©tricas de complejidad (dependen del lenguaje)\n",
    "        complexity_factors = {\n",
    "            'JavaScript': (15, 8), 'Python': (12, 6), 'Java': (20, 10),\n",
    "            'C++': (25, 12), 'C': (28, 15), 'PHP': (18, 9),\n",
    "            'Ruby': (14, 7), 'Go': (16, 8), 'Rust': (22, 11)\n",
    "        }\n",
    "        \n",
    "        base_complexity, complexity_std = complexity_factors[language]\n",
    "        cyclomatic_complexity = max(1, int(np.random.normal(base_complexity, complexity_std)))\n",
    "        \n",
    "        # M√©tricas de seguridad espec√≠ficas por lenguaje\n",
    "        security_metrics = {\n",
    "            'JavaScript': {'has_input_validation': 0.3, 'has_escape_functions': 0.4},\n",
    "            'Python': {'has_input_validation': 0.5, 'has_escape_functions': 0.3},\n",
    "            'Java': {'has_input_validation': 0.6, 'has_escape_functions': 0.5},\n",
    "            'C++': {'has_input_validation': 0.4, 'has_escape_functions': 0.2},\n",
    "            'C': {'has_input_validation': 0.2, 'has_escape_functions': 0.1},\n",
    "            'PHP': {'has_input_validation': 0.3, 'has_escape_functions': 0.3},\n",
    "            'Ruby': {'has_input_validation': 0.5, 'has_escape_functions': 0.4},\n",
    "            'Go': {'has_input_validation': 0.7, 'has_escape_functions': 0.6},\n",
    "            'Rust': {'has_input_validation': 0.8, 'has_escape_functions': 0.7}\n",
    "        }\n",
    "        \n",
    "        metrics = security_metrics[language]\n",
    "        has_input_validation = 1 if random.random() < metrics['has_input_validation'] else 0\n",
    "        has_escape_functions = 1 if random.random() < metrics['has_escape_functions'] else 0\n",
    "        \n",
    "        # Patrones de c√≥digo riesgosos (dependen del tipo de vulnerabilidad)\n",
    "        risky_patterns = {\n",
    "            'SQL_Injection': {'raw_queries': 0.8, 'string_concatenation': 0.7},\n",
    "            'XSS': {'inner_html': 0.9, 'eval_usage': 0.6},\n",
    "            'Buffer_Overflow': {'fixed_size_buffers': 0.8, 'no_bounds_check': 0.9},\n",
    "            'Insecure_Authentication': {'hardcoded_credentials': 0.7, 'weak_hashing': 0.6},\n",
    "            'Path_Traversal': {'user_input_paths': 0.8, 'no_path_validation': 0.7},\n",
    "            'Command_Injection': {'system_calls': 0.9, 'user_input_commands': 0.8},\n",
    "            'XXE': {'xml_parsing': 0.9, 'external_entities': 0.7},\n",
    "            'Deserialization': {'untrusted_deserialization': 0.8, 'no_validation': 0.6},\n",
    "            'Cryptographic_Weakness': {'weak_crypto': 0.7, 'hardcoded_keys': 0.6},\n",
    "            'Secure': {'raw_queries': 0.1, 'string_concatenation': 0.1}\n",
    "        }\n",
    "        \n",
    "        pattern_weights = risky_patterns[vulnerability_type]\n",
    "        has_raw_queries = 1 if random.random() < pattern_weights.get('raw_queries', 0.1) else 0\n",
    "        has_string_concatenation = 1 if random.random() < pattern_weights.get('string_concatenation', 0.1) else 0\n",
    "        has_inner_html = 1 if random.random() < pattern_weights.get('inner_html', 0.1) else 0\n",
    "        \n",
    "        # Historial del desarrollador\n",
    "        developer_experience = np.random.normal(3, 1)  # A√±os de experiencia\n",
    "        previous_vulnerabilities = np.random.poisson(0.5) if is_vulnerable else np.random.poisson(0.1)\n",
    "        \n",
    "        # Tama√±o del cambio (proxy para riesgo)\n",
    "        change_size = lines_added + lines_deleted\n",
    "        risk_factor = min(1.0, change_size / 200)  # Normalizado\n",
    "        \n",
    "        # Generar mensaje de commit realista\n",
    "        commit_message_keywords = random.sample(\n",
    "            vulnerability_keywords[vulnerability_type], \n",
    "            k=min(3, len(vulnerability_keywords[vulnerability_type]))\n",
    "        )\n",
    "        commit_message = f\"Fix: {', '.join(commit_message_keywords)} issues\"\n",
    "        \n",
    "        data.append({\n",
    "            'commit_id': f\"commit_{i:06d}\",\n",
    "            'language': language,\n",
    "            'vulnerability_type': vulnerability_type,\n",
    "            'is_vulnerable': is_vulnerable,\n",
    "            'lines_added': lines_added,\n",
    "            'lines_deleted': lines_deleted,\n",
    "            'files_changed': files_changed,\n",
    "            'cyclomatic_complexity': cyclomatic_complexity,\n",
    "            'has_input_validation': has_input_validation,\n",
    "            'has_escape_functions': has_escape_functions,\n",
    "            'has_raw_queries': has_raw_queries,\n",
    "            'has_string_concatenation': has_string_concatenation,\n",
    "            'has_inner_html': has_inner_html,\n",
    "            'developer_experience': max(0.1, developer_experience),\n",
    "            'previous_vulnerabilities': previous_vulnerabilities,\n",
    "            'change_size': change_size,\n",
    "            'risk_factor': risk_factor,\n",
    "            'commit_message': commit_message,\n",
    "            'timestamp': datetime.now() - timedelta(days=random.randint(0, 365))\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generar el dataset\n",
    "print(\"üîÑ Generando dataset de vulnerabilidades...\")\n",
    "df = generate_vulnerability_dataset(2000)\n",
    "print(f\"Dataset generado: {df.shape[0]} filas, {df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26fc46-c7bd-4a45-b588-7c6b2081dafc",
   "metadata": {},
   "source": [
    "---\n",
    "## An√°lisis Exploratorio de Datos (EDA)\n",
    "Explicaci√≥n: El EDA nos ayuda a:\n",
    "- Entender la distribuci√≥n de los datos\n",
    "- Identificar relaciones entre variables\n",
    "- Detectar posibles problemas de balance en las clases\n",
    "- Validar la calidad del dataset generado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e870a4-15d8-409c-b991-a4389ff8adc2",
   "metadata": {},
   "source": [
    "### An√°lisis Inicial del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdc07b-7421-403a-b778-68c562957ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis inicial del dataset\n",
    "print(\"AN√ÅLISIS EXPLORATORIO DE DATOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nEstructura del Dataset:\")\n",
    "print(f\"Dimensiones: {df.shape}\")\n",
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nInformaci√≥n de columnas:\")\n",
    "print(df.info())\n",
    "\n",
    "print(f\"\\nEstad√≠sticas descriptivas:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(f\"\\nDistribuci√≥n de variables clave:\")\n",
    "print(f\"Lenguajes: \\n{df['language'].value_counts()}\")\n",
    "print(f\"\\nTipos de vulnerabilidad: \\n{df['vulnerability_type'].value_counts()}\")\n",
    "print(f\"\\nBalance vulnerable/seguro: \\n{df['is_vulnerable'].value_counts()}\")\n",
    "print(f\"Proporci√≥n vulnerable: {df['is_vulnerable'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3287d0-bc07-42a9-8218-171defe52f72",
   "metadata": {},
   "source": [
    "### Visualizaciones del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a109b-fe70-4f17-b4dd-f1340e8c9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de la distribuci√≥n de datos\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Distribuci√≥n de lenguajes\n",
    "df['language'].value_counts().plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Distribuci√≥n de Lenguajes de Programaci√≥n')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].set_ylabel('N√∫mero de Commits')\n",
    "\n",
    "# 2. Distribuci√≥n de tipos de vulnerabilidad\n",
    "df['vulnerability_type'].value_counts().plot(kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "axes[0,1].set_title('Distribuci√≥n de Tipos de Vulnerabilidad')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].set_ylabel('Frecuencia')\n",
    "\n",
    "# 3. Distribuci√≥n de complejidad ciclom√°tica\n",
    "df['cyclomatic_complexity'].hist(bins=30, ax=axes[0,2], color='lightgreen', alpha=0.7)\n",
    "axes[0,2].set_title('Distribuci√≥n de Complejidad Ciclom√°tica')\n",
    "axes[0,2].set_xlabel('Complejidad Ciclom√°tica')\n",
    "axes[0,2].set_ylabel('Frecuencia')\n",
    "\n",
    "# 4. Relaci√≥n entre complejidad y vulnerabilidad\n",
    "sns.boxplot(data=df, x='is_vulnerable', y='cyclomatic_complexity', ax=axes[1,0])\n",
    "axes[1,0].set_title('Complejidad vs Vulnerabilidad')\n",
    "axes[1,0].set_xlabel('Es Vulnerable (0=No, 1=S√≠)')\n",
    "axes[1,0].set_ylabel('Complejidad Ciclom√°tica')\n",
    "\n",
    "# 5. Vulnerabilidades por lenguaje\n",
    "vuln_by_lang = df.groupby('language')['is_vulnerable'].mean().sort_values(ascending=False)\n",
    "vuln_by_lang.plot(kind='bar', ax=axes[1,1], color='orange')\n",
    "axes[1,1].set_title('Proporci√≥n de Vulnerabilidades por Lenguaje')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].set_ylabel('Proporci√≥n de Vulnerable')\n",
    "\n",
    "# 6. Tama√±o de cambio vs vulnerabilidad\n",
    "sns.scatterplot(data=df, x='change_size', y='risk_factor', hue='is_vulnerable', \n",
    "                alpha=0.6, ax=axes[1,2])\n",
    "axes[1,2].set_title('Tama√±o de Cambio vs Factor de Riesgo')\n",
    "axes[1,2].set_xlabel('Tama√±o del Cambio (l√≠neas)')\n",
    "axes[1,2].set_ylabel('Factor de Riesgo')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de correlaciones\n",
    "print(\"\\nüîó Matriz de Correlaciones:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Matriz de Correlaci√≥n de Caracter√≠sticas Num√©ricas')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714476c-cc7b-4158-a52c-e8699fb26723",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocesamiento de Datos\n",
    "**Preparaci√≥n de Caracter√≠sticas**<br>\n",
    "Explicaci√≥n: Esta funci√≥n:\n",
    "- Separa caracter√≠sticas de etiquetas\n",
    "- Codifica las etiquetas de texto a num√©ricas para modelos de ML\n",
    "- Define el preprocesador para diferentes tipos de caracter√≠sticas\n",
    "- Aplica escalado a num√©ricas y one-hot encoding a categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3fb53-d616-4d95-93e6-1e253889d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepara las caracter√≠sticas para el modelado\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X, y_binary, y_multiclass, preprocessor, label_encoder)\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Preparando caracter√≠sticas para modelado...\")\n",
    "    \n",
    "    # Separar caracter√≠sticas y targets\n",
    "    X = df.drop(['commit_id', 'vulnerability_type', 'is_vulnerable', 'commit_message', 'timestamp'], axis=1)\n",
    "    y_binary = df['is_vulnerable']  # Clasificaci√≥n binaria\n",
    "    y_multiclass = df['vulnerability_type']  # Clasificaci√≥n multiclase\n",
    "    \n",
    "    # Codificar etiquetas multiclase a n√∫meros\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_multiclass_encoded = label_encoder.fit_transform(y_multiclass)\n",
    "    \n",
    "    print(\"Mapeo de clases de vulnerabilidad:\")\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        print(f\"  {i}: {class_name}\")\n",
    "    \n",
    "    # Definir transformadores\n",
    "    numeric_features = [\n",
    "        'lines_added', 'lines_deleted', 'files_changed', 'cyclomatic_complexity',\n",
    "        'developer_experience', 'previous_vulnerabilities', 'change_size', 'risk_factor'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = ['language']\n",
    "    \n",
    "    binary_features = [\n",
    "        'has_input_validation', 'has_escape_functions', 'has_raw_queries',\n",
    "        'has_string_concatenation', 'has_inner_html'\n",
    "    ]\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features),\n",
    "            ('bin', 'passthrough', binary_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"Caracter√≠sticas preparadas exitosamente\")\n",
    "    return X, y_binary, y_multiclass_encoded, preprocessor, label_encoder\n",
    "\n",
    "# Preparar los datos\n",
    "X, y_binary, y_multiclass_encoded, preprocessor, label_encoder = prepare_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0cdf0d-28fb-40e0-b2e9-5132720079ac",
   "metadata": {},
   "source": [
    "**Divisi√≥n de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb1c93-58b9-408a-b89f-1c4f50e75f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "print(\"üîÑ Dividiendo datos en train y test...\")\n",
    "\n",
    "# Para clasificaci√≥n binaria\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "# Para clasificaci√≥n multiclase\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X, y_multiclass_encoded, test_size=0.2, random_state=42, stratify=y_multiclass_encoded\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Datos divididos exitosamente:\")\n",
    "print(f\"üìä Conjunto de entrenamiento binario: {X_train_bin.shape}\")\n",
    "print(f\"üìä Conjunto de prueba binario: {X_test_bin.shape}\")\n",
    "print(f\"üéØ Conjunto de entrenamiento multiclase: {X_train_multi.shape}\")\n",
    "print(f\"üéØ Conjunto de prueba multiclase: {X_test_multi.shape}\")\n",
    "\n",
    "# Verificar balance de clases\n",
    "print(f\"\\n‚öñÔ∏è Balance en entrenamiento binario:\")\n",
    "print(pd.Series(y_train_bin).value_counts(normalize=True))\n",
    "print(f\"\\n‚öñÔ∏è Balance en entrenamiento multiclase:\")\n",
    "unique, counts = np.unique(y_train_multi, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    class_name = label_encoder.inverse_transform([cls])[0]\n",
    "    print(f\"  {class_name}: {count/len(y_train_multi):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18121aa3-36c5-4ee5-83ff-5c8ed7808d50",
   "metadata": {},
   "source": [
    "---\n",
    "### Entrenamiento de Modelos\n",
    "Explicaci√≥n: Entrenamos m√∫ltiples algoritmos para:\n",
    "- Binario: Random Forest, Logistic Regression, SVM, XGBoost\n",
    "- Multiclase: Random Forest y XGBoost. <br>\n",
    "  \n",
    "Cada modelo se eval√∫a con m√©tricas est√°ndar de clasificaci√≥n.\n",
    "#### Clasificaci√≥n Binaria (Seguro/Vulnerable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6e517-77f8-4dd9-b4ea-b64ec0003ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos para clasificaci√≥n binaria\n",
    "print(\"üîß Entrenando modelos de clasificaci√≥n binaria...\")\n",
    "\n",
    "binary_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "binary_results = {}\n",
    "\n",
    "for name, model in binary_models.items():\n",
    "    print(f\"üîÑ Entrenando {name}...\")\n",
    "    \n",
    "    # Crear pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    pipeline.fit(X_train_bin, y_train_bin)\n",
    "    \n",
    "    # Predecir\n",
    "    y_pred = pipeline.predict(X_test_bin)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test_bin)[:, 1]\n",
    "    \n",
    "    # Evaluar\n",
    "    accuracy = accuracy_score(y_test_bin, y_pred)\n",
    "    binary_results[name] = {\n",
    "        'model': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name} - Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test_bin, y_pred))\n",
    "    print(\"‚îÄ\" * 50)\n",
    "\n",
    "# Mostrar comparaci√≥n de modelos binarios\n",
    "print(\"\\nüèÜ COMPARACI√ìN DE MODELOS BINARIOS:\")\n",
    "binary_comparison = pd.DataFrame({\n",
    "    'Modelo': list(binary_results.keys()),\n",
    "    'Accuracy': [result['accuracy'] for result in binary_results.values()]\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(binary_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b715d-3af5-4afc-bffd-d5a2d05b4598",
   "metadata": {},
   "source": [
    "#### Clasificaci√≥n Multiclase (Tipo de Vulnerabilidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4b19a-ca2b-4ac1-90e6-413fef082666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos para clasificaci√≥n multiclase\n",
    "print(\"üîß Entrenando modelos de clasificaci√≥n multiclase...\")\n",
    "\n",
    "multi_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "multi_results = {}\n",
    "\n",
    "for name, model in multi_models.items():\n",
    "    print(f\"üîÑ Entrenando {name}...\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    pipeline.fit(X_train_multi, y_train_multi)\n",
    "    \n",
    "    # Predecir\n",
    "    y_pred = pipeline.predict(X_test_multi)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test_multi)\n",
    "    \n",
    "    # Evaluar\n",
    "    accuracy = accuracy_score(y_test_multi, y_pred)\n",
    "    multi_results[name] = {\n",
    "        'model': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'label_encoder': label_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Convertir n√∫meros de vuelta a nombres para el reporte\n",
    "    y_test_names = label_encoder.inverse_transform(y_test_multi)\n",
    "    y_pred_names = label_encoder.inverse_transform(y_pred)\n",
    "    \n",
    "    print(classification_report(y_test_names, y_pred_names))\n",
    "    print(\"‚îÄ\" * 50)\n",
    "\n",
    "# Mostrar comparaci√≥n de modelos multiclase\n",
    "print(\"\\nüèÜ COMPARACI√ìN DE MODELOS MULTICLASE:\")\n",
    "multi_comparison = pd.DataFrame({\n",
    "    'Modelo': list(multi_results.keys()),\n",
    "    'Accuracy': [result['accuracy'] for result in multi_results.values()]\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(multi_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1982fa-a2c8-4359-b4b1-409207ee9886",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluaci√≥n y Visualizaci√≥n\n",
    "#### Matrices de Confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d6c03-387a-4045-a5c5-1873d4f8ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcci√≥n de matrices de confusi√≥n\n",
    "print(\"üìà Visualizando matrices de confusi√≥n...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Matriz de confusi√≥n para el mejor modelo binario\n",
    "best_binary_name = binary_comparison.iloc[0]['Modelo']\n",
    "best_binary_result = binary_results[best_binary_name]\n",
    "\n",
    "# Calcular matriz de confusi√≥n binaria\n",
    "cm_binary = confusion_matrix(y_test_bin, best_binary_result['predictions'])\n",
    "\n",
    "# Visualizar matriz binaria\n",
    "sns.heatmap(cm_binary, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
    "            xticklabels=['Seguro (0)', 'Vulnerable (1)'],\n",
    "            yticklabels=['Seguro (0)', 'Vulnerable (1)'])\n",
    "axes[0,0].set_title(f'Matriz de Confusi√≥n - {best_binary_name} (Binario)')\n",
    "axes[0,0].set_xlabel('Predicci√≥n')\n",
    "axes[0,0].set_ylabel('Real')\n",
    "\n",
    "# 2. Matriz de confusi√≥n para el mejor modelo multiclase\n",
    "best_multi_name = multi_comparison.iloc[0]['Modelo']\n",
    "best_multi_result = multi_results[best_multi_name]\n",
    "\n",
    "# Convertir etiquetas num√©ricas a nombres para la matriz\n",
    "y_test_multi_names = label_encoder.inverse_transform(y_test_multi)\n",
    "y_pred_multi_names = label_encoder.inverse_transform(best_multi_result['predictions'])\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Calcular matriz de confusi√≥n multiclase\n",
    "cm_multi = confusion_matrix(y_test_multi_names, y_pred_multi_names, labels=class_names)\n",
    "\n",
    "# Visualizar matriz multiclase\n",
    "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues', ax=axes[0,1],\n",
    "            xticklabels=class_names, \n",
    "            yticklabels=class_names)\n",
    "axes[0,1].set_title(f'Matriz de Confusi√≥n - {best_multi_name} (Multiclase)')\n",
    "axes[0,1].set_xlabel('Predicci√≥n')\n",
    "axes[0,1].set_ylabel('Real')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# 3. Comparaci√≥n de accuracy entre modelos\n",
    "models_combined = {\n",
    "    **{f\"Binario - {k}\": v['accuracy'] for k, v in binary_results.items()},\n",
    "    **{f\"Multiclase - {k}\": v['accuracy'] for k, v in multi_results.items()}\n",
    "}\n",
    "\n",
    "models_df = pd.DataFrame({\n",
    "    'Modelo': list(models_combined.keys()),\n",
    "    'Accuracy': list(models_combined.values())\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "sns.barplot(data=models_df, x='Accuracy', y='Modelo', ax=axes[1,0], palette='viridis')\n",
    "axes[1,0].set_title('Comparaci√≥n de Accuracy entre Modelos')\n",
    "axes[1,0].set_xlim(0, 1)\n",
    "\n",
    "# 4. Importancia de caracter√≠sticas - CORREGIDO\n",
    "try:\n",
    "    best_model = binary_results[best_binary_name]['model']\n",
    "    \n",
    "    # Obtener nombres de caracter√≠sticas del preprocesador\n",
    "    feature_names = []\n",
    "    \n",
    "    # Caracter√≠sticas num√©ricas\n",
    "    numeric_features = [\n",
    "        'lines_added', 'lines_deleted', 'files_changed', 'cyclomatic_complexity',\n",
    "        'developer_experience', 'previous_vulnerabilities', 'change_size', 'risk_factor'\n",
    "    ]\n",
    "    feature_names.extend(numeric_features)\n",
    "    \n",
    "    # Caracter√≠sticas categ√≥ricas (one-hot encoded)\n",
    "    if hasattr(preprocessor.named_transformers_['cat'], 'get_feature_names_out'):\n",
    "        cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(['language'])\n",
    "        feature_names.extend(cat_features)\n",
    "    else:\n",
    "        # Fallback: nombres gen√©ricos para caracter√≠sticas categ√≥ricas\n",
    "        unique_langs = X['language'].unique()\n",
    "        feature_names.extend([f'language_{lang}' for lang in unique_langs[1:]])  # Excluir primera categor√≠a (drop='first')\n",
    "    \n",
    "    # Caracter√≠sticas binarias\n",
    "    binary_features = [\n",
    "        'has_input_validation', 'has_escape_functions', 'has_raw_queries',\n",
    "        'has_string_concatenation', 'has_inner_html'\n",
    "    ]\n",
    "    feature_names.extend(binary_features)\n",
    "    \n",
    "    # Obtener importancias del modelo\n",
    "    if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "        importances = best_model.named_steps['classifier'].feature_importances_\n",
    "        \n",
    "        # Asegurarse de que tenemos el mismo n√∫mero de caracter√≠sticas\n",
    "        min_length = min(len(importances), len(feature_names))\n",
    "        importances = importances[:min_length]\n",
    "        feature_names_used = feature_names[:min_length]\n",
    "        \n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names_used,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False).head(10)\n",
    "        \n",
    "        sns.barplot(data=feature_importance_df, x='importance', y='feature', ax=axes[1,1], palette='rocket')\n",
    "        axes[1,1].set_title('Top 10 Caracter√≠sticas M√°s Importantes')\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'Importancias no disponibles\\npara este modelo', \n",
    "                      ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "        axes[1,1].set_title('Importancia de Caracter√≠sticas')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error al generar importancia de caracter√≠sticas: {e}\")\n",
    "    axes[1,1].text(0.5, 0.5, f'Error: {str(e)}', \n",
    "                  ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "    axes[1,1].set_title('Importancia de Caracter√≠sticas')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizaciones completadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891b54d-8582-4f21-99de-8149745e8567",
   "metadata": {},
   "source": [
    "#### An√°lisis de Caracter√≠sticas Importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2c310-4662-4e2d-b1f0-4908c13b7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AN√ÅLISIS DE CARACTER√çSTICAS IMPORTANTES - VERSI√ìN CORREGIDA\n",
    "print(\"üîç AN√ÅLISIS DE CARACTER√çSTICAS IMPORTANTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def get_feature_importance_analysis(model, preprocessor, feature_names, top_n=15):\n",
    "    \"\"\"\n",
    "    Funci√≥n robusta para analizar la importancia de caracter√≠sticas\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado con pipeline\n",
    "        preprocessor: Preprocesador usado\n",
    "        feature_names: Lista de nombres originales de caracter√≠sticas\n",
    "        top_n: N√∫mero top de caracter√≠sticas a mostrar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con importancia de caracter√≠sticas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verificar si el modelo tiene importancias\n",
    "        if not hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
    "            print(\"‚ö†Ô∏è Este modelo no tiene importancias de caracter√≠sticas\")\n",
    "            return None\n",
    "        \n",
    "        # Obtener importancias\n",
    "        importances = model.named_steps['classifier'].feature_importances_\n",
    "        \n",
    "        # Construir nombres de caracter√≠sticas del preprocesador\n",
    "        final_feature_names = []\n",
    "        \n",
    "        # 1. Caracter√≠sticas num√©ricas (mantienen sus nombres)\n",
    "        numeric_features = [\n",
    "            'lines_added', 'lines_deleted', 'files_changed', 'cyclomatic_complexity',\n",
    "            'developer_experience', 'previous_vulnerabilities', 'change_size', 'risk_factor'\n",
    "        ]\n",
    "        final_feature_names.extend(numeric_features)\n",
    "        \n",
    "        # 2. Caracter√≠sticas categ√≥ricas (OneHot Encoding)\n",
    "        categorical_transformer = preprocessor.named_transformers_['cat']\n",
    "        if hasattr(categorical_transformer, 'get_feature_names_out'):\n",
    "            cat_features = categorical_transformer.get_feature_names_out(['language'])\n",
    "            final_feature_names.extend(cat_features)\n",
    "        else:\n",
    "            # Fallback manual\n",
    "            unique_langs = ['JavaScript', 'Python', 'Java', 'C++', 'C', 'PHP', 'Ruby', 'Go', 'Rust']\n",
    "            # Excluir la primera categor√≠a (due to drop='first')\n",
    "            final_feature_names.extend([f'language_{lang}' for lang in unique_langs[1:]])\n",
    "        \n",
    "        # 3. Caracter√≠sticas binarias\n",
    "        binary_features = [\n",
    "            'has_input_validation', 'has_escape_functions', 'has_raw_queries',\n",
    "            'has_string_concatenation', 'has_inner_html'\n",
    "        ]\n",
    "        final_feature_names.extend(binary_features)\n",
    "        \n",
    "        # Asegurar que tenemos la misma cantidad de caracter√≠sticas\n",
    "        min_length = min(len(importances), len(final_feature_names))\n",
    "        importances = importances[:min_length]\n",
    "        final_feature_names = final_feature_names[:min_length]\n",
    "        \n",
    "        # Crear DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': final_feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False).head(top_n)\n",
    "        \n",
    "        return importance_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en an√°lisis de caracter√≠sticas: {e}\")\n",
    "        return None\n",
    "\n",
    "# Aplicar a todos los modelos que tengan importancias\n",
    "print(\"üìä IMPORTANCIA DE CARACTER√çSTICAS POR MODELO\")\n",
    "\n",
    "# Para modelos binarios\n",
    "print(\"\\nüîí MODELOS BINARIOS:\")\n",
    "binary_importance_results = {}\n",
    "\n",
    "for model_name, result in binary_results.items():\n",
    "    print(f\"\\nüîπ Analizando {model_name}...\")\n",
    "    \n",
    "    importance_df = get_feature_importance_analysis(\n",
    "        result['model'], \n",
    "        preprocessor,\n",
    "        list(X.columns),\n",
    "        top_n=10\n",
    "    )\n",
    "    \n",
    "    if importance_df is not None:\n",
    "        binary_importance_results[model_name] = importance_df\n",
    "        print(f\"‚úÖ Top 5 caracter√≠sticas m√°s importantes:\")\n",
    "        for idx, row in importance_df.head().iterrows():\n",
    "            print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No se pudieron obtener importancias para {model_name}\")\n",
    "\n",
    "# Para modelos multiclase\n",
    "print(\"\\nüéØ MODELOS MULTICLASE:\")\n",
    "multi_importance_results = {}\n",
    "\n",
    "for model_name, result in multi_results.items():\n",
    "    print(f\"\\nüîπ Analizando {model_name}...\")\n",
    "    \n",
    "    importance_df = get_feature_importance_analysis(\n",
    "        result['model'], \n",
    "        preprocessor,\n",
    "        list(X.columns),\n",
    "        top_n=10\n",
    "    )\n",
    "    \n",
    "    if importance_df is not None:\n",
    "        multi_importance_results[model_name] = importance_df\n",
    "        print(f\"‚úÖ Top 5 caracter√≠sticas m√°s importantes:\")\n",
    "        for idx, row in importance_df.head().iterrows():\n",
    "            print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No se pudieron obtener importancias para {model_name}\")\n",
    "\n",
    "# Visualizaci√≥n de importancias\n",
    "print(\"\\nüìà VISUALIZANDO IMPORTANCIAS...\")\n",
    "\n",
    "# Seleccionar el mejor modelo para visualizaci√≥n detallada\n",
    "if binary_importance_results:\n",
    "    best_binary_model_name = max(binary_results.items(), \n",
    "                                key=lambda x: x[1]['accuracy'])[0]\n",
    "    \n",
    "    if best_binary_model_name in binary_importance_results:\n",
    "        best_importance_df = binary_importance_results[best_binary_model_name]\n",
    "        \n",
    "        # Crear visualizaci√≥n\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Gr√°fico de barras horizontal\n",
    "        sns.barplot(data=best_importance_df, x='importance', y='feature', \n",
    "                   ax=ax1, palette='viridis')\n",
    "        ax1.set_title(f'Importancia de Caracter√≠sticas - {best_binary_model_name}\\n(Clasificaci√≥n Binaria)')\n",
    "        ax1.set_xlabel('Importancia')\n",
    "        ax1.set_ylabel('Caracter√≠stica')\n",
    "        \n",
    "        # Gr√°fico de pie para top 5\n",
    "        top_5 = best_importance_df.head(5)\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(top_5)))\n",
    "        ax2.pie(top_5['importance'], labels=top_5['feature'], autopct='%1.1f%%',\n",
    "               colors=colors, startangle=90)\n",
    "        ax2.set_title('Distribuci√≥n de Importancia\\n(Top 5 Caracter√≠sticas)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # An√°lisis detallado\n",
    "        print(f\"\\nüìã AN√ÅLISIS DETALLADO - {best_binary_model_name}:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for idx, row in best_importance_df.iterrows():\n",
    "            importance_percent = (row['importance'] / best_importance_df['importance'].sum()) * 100\n",
    "            print(f\"#{idx+1:2d} {row['feature']:30} {row['importance']:.4f} ({importance_percent:5.1f}%)\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No hay datos de importancia para el mejor modelo binario\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay resultados de importancia disponibles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe7863-8c10-4099-87fb-8370bb3aaf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AN√ÅLISIS POR TIPO DE CARACTER√çSTICA\n",
    "print(\"üéØ AN√ÅLISIS POR TIPO DE CARACTER√çSTICA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def analyze_feature_categories(importance_df):\n",
    "    \"\"\"Analiza las caracter√≠sticas por categor√≠as\"\"\"\n",
    "    if importance_df is None:\n",
    "        return\n",
    "    \n",
    "    # Definir categor√≠as\n",
    "    categories = {\n",
    "        'Complejidad y Tama√±o': ['cyclomatic_complexity', 'change_size', 'files_changed', \n",
    "                                'lines_added', 'lines_deleted'],\n",
    "        'Historial Desarrollador': ['developer_experience', 'previous_vulnerabilities', 'risk_factor'],\n",
    "        'Pr√°cticas Seguras': ['has_input_validation', 'has_escape_functions'],\n",
    "        'Patrones Riesgosos': ['has_raw_queries', 'has_string_concatenation', 'has_inner_html'],\n",
    "        'Lenguaje': [col for col in importance_df['feature'] if col.startswith('language_')]\n",
    "    }\n",
    "    \n",
    "    category_importance = {}\n",
    "    \n",
    "    for category, features in categories.items():\n",
    "        category_features = importance_df[importance_df['feature'].isin(features)]\n",
    "        total_importance = category_features['importance'].sum()\n",
    "        category_importance[category] = {\n",
    "            'total_importance': total_importance,\n",
    "            'feature_count': len(category_features),\n",
    "            'features': category_features.to_dict('records')\n",
    "        }\n",
    "    \n",
    "    return category_importance\n",
    "\n",
    "# Aplicar an√°lisis al mejor modelo\n",
    "if binary_importance_results and best_binary_model_name in binary_importance_results:\n",
    "    best_importance_df = binary_importance_results[best_binary_model_name]\n",
    "    category_analysis = analyze_feature_categories(best_importance_df)\n",
    "    \n",
    "    if category_analysis:\n",
    "        print(f\"\\nüìä DISTRIBUCI√ìN POR CATEGOR√çAS - {best_binary_model_name}:\")\n",
    "        \n",
    "        # Calcular porcentajes\n",
    "        total_importance = best_importance_df['importance'].sum()\n",
    "        \n",
    "        for category, data in category_analysis.items():\n",
    "            percentage = (data['total_importance'] / total_importance) * 100\n",
    "            print(f\"\\nüîπ {category}:\")\n",
    "            print(f\"   Importancia total: {data['total_importance']:.4f} ({percentage:.1f}%)\")\n",
    "            print(f\"   N√∫mero de caracter√≠sticas: {data['feature_count']}\")\n",
    "            \n",
    "            if data['features']:\n",
    "                print(\"   Caracter√≠sticas incluidas:\")\n",
    "                for feature in data['features']:\n",
    "                    feat_percentage = (feature['importance'] / total_importance) * 100\n",
    "                    print(f\"     - {feature['feature']}: {feature['importance']:.4f} ({feat_percentage:.1f}%)\")\n",
    "\n",
    "# Visualizaci√≥n de categor√≠as\n",
    "if binary_importance_results and best_binary_model_name in binary_importance_results:\n",
    "    category_analysis = analyze_feature_categories(best_importance_df)\n",
    "    \n",
    "    if category_analysis:\n",
    "        # Preparar datos para visualizaci√≥n\n",
    "        categories = list(category_analysis.keys())\n",
    "        importances = [data['total_importance'] for data in category_analysis.values()]\n",
    "        percentages = [(imp / total_importance) * 100 for imp in importances]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Gr√°fico de barras\n",
    "        bars = ax1.bar(categories, importances, color=plt.cm.Pastel1(range(len(categories))))\n",
    "        ax1.set_title('Importancia por Categor√≠a de Caracter√≠stica')\n",
    "        ax1.set_ylabel('Importancia Total')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # A√±adir valores en las barras\n",
    "        for bar, percentage in zip(bars, percentages):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{percentage:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        # Gr√°fico de pie\n",
    "        ax2.pie(importances, labels=categories, autopct='%1.1f%%', startangle=90,\n",
    "               colors=plt.cm.Set3(np.linspace(0, 1, len(categories))))\n",
    "        ax2.set_title('Distribuci√≥n de Importancia por Categor√≠a')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ca50f-80ab-465e-a1f9-1410a1a8ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AN√ÅLISIS POR TIPO DE CARACTER√çSTICA\n",
    "print(\"üéØ AN√ÅLISIS POR TIPO DE CARACTER√çSTICA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def analyze_feature_categories(importance_df):\n",
    "    \"\"\"Analiza las caracter√≠sticas por categor√≠as\"\"\"\n",
    "    if importance_df is None:\n",
    "        return\n",
    "    \n",
    "    # Definir categor√≠as\n",
    "    categories = {\n",
    "        'Complejidad y Tama√±o': ['cyclomatic_complexity', 'change_size', 'files_changed', \n",
    "                                'lines_added', 'lines_deleted'],\n",
    "        'Historial Desarrollador': ['developer_experience', 'previous_vulnerabilities', 'risk_factor'],\n",
    "        'Pr√°cticas Seguras': ['has_input_validation', 'has_escape_functions'],\n",
    "        'Patrones Riesgosos': ['has_raw_queries', 'has_string_concatenation', 'has_inner_html'],\n",
    "        'Lenguaje': [col for col in importance_df['feature'] if col.startswith('language_')]\n",
    "    }\n",
    "    \n",
    "    category_importance = {}\n",
    "    \n",
    "    for category, features in categories.items():\n",
    "        category_features = importance_df[importance_df['feature'].isin(features)]\n",
    "        total_importance = category_features['importance'].sum()\n",
    "        category_importance[category] = {\n",
    "            'total_importance': total_importance,\n",
    "            'feature_count': len(category_features),\n",
    "            'features': category_features.to_dict('records')\n",
    "        }\n",
    "    \n",
    "    return category_importance\n",
    "\n",
    "# Aplicar an√°lisis al mejor modelo\n",
    "if binary_importance_results and best_binary_model_name in binary_importance_results:\n",
    "    best_importance_df = binary_importance_results[best_binary_model_name]\n",
    "    category_analysis = analyze_feature_categories(best_importance_df)\n",
    "    \n",
    "    if category_analysis:\n",
    "        print(f\"\\nüìä DISTRIBUCI√ìN POR CATEGOR√çAS - {best_binary_model_name}:\")\n",
    "        \n",
    "        # Calcular porcentajes\n",
    "        total_importance = best_importance_df['importance'].sum()\n",
    "        \n",
    "        for category, data in category_analysis.items():\n",
    "            percentage = (data['total_importance'] / total_importance) * 100\n",
    "            print(f\"\\nüîπ {category}:\")\n",
    "            print(f\"   Importancia total: {data['total_importance']:.4f} ({percentage:.1f}%)\")\n",
    "            print(f\"   N√∫mero de caracter√≠sticas: {data['feature_count']}\")\n",
    "            \n",
    "            if data['features']:\n",
    "                print(\"   Caracter√≠sticas incluidas:\")\n",
    "                for feature in data['features']:\n",
    "                    feat_percentage = (feature['importance'] / total_importance) * 100\n",
    "                    print(f\"     - {feature['feature']}: {feature['importance']:.4f} ({feat_percentage:.1f}%)\")\n",
    "\n",
    "# Visualizaci√≥n de categor√≠as\n",
    "if binary_importance_results and best_binary_model_name in binary_importance_results:\n",
    "    category_analysis = analyze_feature_categories(best_importance_df)\n",
    "    \n",
    "    if category_analysis:\n",
    "        # Preparar datos para visualizaci√≥n\n",
    "        categories = list(category_analysis.keys())\n",
    "        importances = [data['total_importance'] for data in category_analysis.values()]\n",
    "        percentages = [(imp / total_importance) * 100 for imp in importances]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Gr√°fico de barras\n",
    "        bars = ax1.bar(categories, importances, color=plt.cm.Pastel1(range(len(categories))))\n",
    "        ax1.set_title('Importancia por Categor√≠a de Caracter√≠stica')\n",
    "        ax1.set_ylabel('Importancia Total')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # A√±adir valores en las barras\n",
    "        for bar, percentage in zip(bars, percentages):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{percentage:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        # Gr√°fico de pie\n",
    "        ax2.pie(importances, labels=categories, autopct='%1.1f%%', startangle=90,\n",
    "               colors=plt.cm.Set3(np.linspace(0, 1, len(categories))))\n",
    "        ax2.set_title('Distribuci√≥n de Importancia por Categor√≠a')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9115ac-9285-46a2-934c-885dc07388c7",
   "metadata": {},
   "source": [
    "---\n",
    "# Sistema de Predicci√≥n\n",
    "#### Funci√≥n de Predicci√≥n para Nuevos Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534771f-47f0-4cbd-88fb-64cf4f0e0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_commit_security(binary_model, multi_model, label_encoder, commit_data):\n",
    "    \"\"\"\n",
    "    Predice seguridad y tipo de vulnerabilidad para un nuevo commit\n",
    "    \n",
    "    Args:\n",
    "        binary_model: Modelo entrenado para clasificaci√≥n binaria\n",
    "        multi_model: Modelo entrenado para clasificaci√≥n multiclase  \n",
    "        label_encoder: Encoder para etiquetas multiclase\n",
    "        commit_data: Diccionario con caracter√≠sticas del commit\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultados de la predicci√≥n con probabilidades\n",
    "    \"\"\"\n",
    "    # Crear DataFrame\n",
    "    new_commit_df = pd.DataFrame([commit_data])\n",
    "    \n",
    "    # Predicci√≥n binaria\n",
    "    is_vulnerable_pred = binary_model.predict(new_commit_df)[0]\n",
    "    vulnerability_prob = binary_model.predict_proba(new_commit_df)[0, 1]\n",
    "    \n",
    "    # Predicci√≥n multiclase\n",
    "    vulnerability_type_encoded = multi_model.predict(new_commit_df)[0]\n",
    "    vulnerability_type_proba = multi_model.predict_proba(new_commit_df)[0]\n",
    "    vulnerability_type_name = label_encoder.inverse_transform([vulnerability_type_encoded])[0]\n",
    "    \n",
    "    # Obtener la probabilidad de la clase predicha\n",
    "    max_prob = vulnerability_type_proba[vulnerability_type_encoded]\n",
    "    \n",
    "    result = {\n",
    "        'is_vulnerable': bool(is_vulnerable_pred),\n",
    "        'vulnerability_probability': float(vulnerability_prob),\n",
    "        'vulnerability_type': vulnerability_type_name,\n",
    "        'vulnerability_type_confidence': float(max_prob),\n",
    "        'risk_level': 'ALTO' if vulnerability_prob > 0.7 else \n",
    "                     'MEDIO' if vulnerability_prob > 0.3 else 'BAJO',\n",
    "        'all_probabilities': {\n",
    "            label_encoder.inverse_transform([i])[0]: float(prob) \n",
    "            for i, prob in enumerate(vulnerability_type_proba)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(\"üéØ Ejemplo de predicci√≥n para nuevo commit:\")\n",
    "\n",
    "example_commit = {\n",
    "    'language': 'Python',\n",
    "    'lines_added': 45,\n",
    "    'lines_deleted': 12,\n",
    "    'files_changed': 2,\n",
    "    'cyclomatic_complexity': 18,\n",
    "    'has_input_validation': 0,\n",
    "    'has_escape_functions': 0,\n",
    "    'has_raw_queries': 1,\n",
    "    'has_string_concatenation': 1,\n",
    "    'has_inner_html': 0,\n",
    "    'developer_experience': 1.5,\n",
    "    'previous_vulnerabilities': 2,\n",
    "    'change_size': 57,\n",
    "    'risk_factor': 0.4\n",
    "}\n",
    "\n",
    "# Obtener los mejores modelos\n",
    "best_binary_model = binary_results[best_binary_name]['model']\n",
    "best_multi_model = multi_results[best_multi_name]['model']\n",
    "best_label_encoder = multi_results[best_multi_name]['label_encoder']\n",
    "\n",
    "prediction = predict_commit_security(\n",
    "    best_binary_model, \n",
    "    best_multi_model, \n",
    "    best_label_encoder, \n",
    "    example_commit\n",
    ")\n",
    "\n",
    "print(\"üîç Predicci√≥n Completa para Nuevo Commit:\")\n",
    "print(f\"  ‚ö†Ô∏è  Es vulnerable: {prediction['is_vulnerable']}\")\n",
    "print(f\"  üìä Probabilidad de vulnerabilidad: {prediction['vulnerability_probability']:.2%}\")\n",
    "print(f\"  üéØ Tipo de vulnerabilidad: {prediction['vulnerability_type']}\")\n",
    "print(f\"  ‚úÖ Confianza en el tipo: {prediction['vulnerability_type_confidence']:.2%}\")\n",
    "print(f\"  üö¶ Nivel de riesgo: {prediction['risk_level']}\")\n",
    "\n",
    "print(\"\\nüìà Probabilidades por tipo de vulnerabilidad:\")\n",
    "for vuln_type, prob in sorted(prediction['all_probabilities'].items(), \n",
    "                             key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {vuln_type}: {prob:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a84f8-43df-4284-8ffa-3fcc8002e748",
   "metadata": {},
   "source": [
    "# Guardado de Modelos y Resultados\n",
    "#### Persistencia de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc330f1-cb53-4e18-8869-ed7defb381dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ GUARDADO DE MODELOS Y RESULTADOS - VERSI√ìN CORREGIDA\n",
    "print(\"üíæ Guardando modelos y resultados...\")\n",
    "\n",
    "import os\n",
    "# Crear directorio si no existe\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Guardar dataset\n",
    "df.to_csv('models/github_commits_vulnerability_dataset.csv', index=False)\n",
    "\n",
    "# Guardar modelos binarios\n",
    "for name, result in binary_results.items():\n",
    "    filename = f'models/binary_{name.lower().replace(\" \", \"_\")}.pkl'\n",
    "    joblib.dump(result['model'], filename)\n",
    "\n",
    "# Guardar modelos multiclase\n",
    "for name, result in multi_results.items():\n",
    "    model_data = {\n",
    "        'model': result['model'],\n",
    "        'label_encoder': result['label_encoder']\n",
    "    }\n",
    "    filename = f'models/multi_{name.lower().replace(\" \", \"_\")}.pkl'\n",
    "    joblib.dump(model_data, filename)\n",
    "\n",
    "# Guardar preprocessor y label encoder\n",
    "joblib.dump(preprocessor, 'models/feature_preprocessor.pkl')\n",
    "joblib.dump(label_encoder, 'models/label_encoder.pkl')\n",
    "\n",
    "# PREPARAR RESULTADOS DE EVALUACI√ìN DE MANERA SEGURA\n",
    "print(\"üìä Preparando resumen de resultados...\")\n",
    "\n",
    "# Inicializar diccionario de resultados\n",
    "results_summary = {\n",
    "    'binary_results': {k: v['accuracy'] for k, v in binary_results.items()},\n",
    "    'multi_results': {k: v['accuracy'] for k, v in multi_results.items()},\n",
    "    'dataset_info': {\n",
    "        'shape': df.shape,\n",
    "        'vulnerable_ratio': float(df['is_vulnerable'].mean()),\n",
    "        'languages': int(df['language'].nunique()),\n",
    "        'vulnerability_types': int(df['vulnerability_type'].nunique())\n",
    "    }\n",
    "}\n",
    "\n",
    "# A√±adir importancia de caracter√≠sticas si est√° disponible\n",
    "try:\n",
    "    # Intentar obtener la importancia del mejor modelo binario\n",
    "    if 'binary_importance_results' in globals() and binary_importance_results:\n",
    "        best_binary_model_name = max(binary_results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "        if best_binary_model_name in binary_importance_results:\n",
    "            feature_importance_data = binary_importance_results[best_binary_model_name].to_dict()\n",
    "            results_summary['feature_importance'] = feature_importance_data\n",
    "            print(\"‚úÖ Importancia de caracter√≠sticas incluida en el resumen\")\n",
    "        else:\n",
    "            results_summary['feature_importance'] = {}\n",
    "            print(\"‚ö†Ô∏è No se pudo obtener importancia del mejor modelo binario\")\n",
    "    else:\n",
    "        results_summary['feature_importance'] = {}\n",
    "        print(\"‚ö†Ô∏è An√°lisis de importancia no disponible\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error al guardar importancia de caracter√≠sticas: {e}\")\n",
    "    results_summary['feature_importance'] = {}\n",
    "\n",
    "# A√±adir informaci√≥n de los mejores modelos\n",
    "try:\n",
    "    best_binary_name = max(binary_results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "    best_multi_name = max(multi_results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "    \n",
    "    results_summary['best_models'] = {\n",
    "        'binary': best_binary_name,\n",
    "        'multiclass': best_multi_name,\n",
    "        'binary_accuracy': float(binary_results[best_binary_name]['accuracy']),\n",
    "        'multiclass_accuracy': float(multi_results[best_multi_name]['accuracy'])\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ Informaci√≥n de mejores modelos incluida\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error al guardar informaci√≥n de mejores modelos: {e}\")\n",
    "    results_summary['best_models'] = {}\n",
    "\n",
    "# Guardar resumen de resultados\n",
    "joblib.dump(results_summary, 'models/results_summary.pkl')\n",
    "\n",
    "# Tambi√©n guardar como JSON para f√°cil lectura\n",
    "import json\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convierte objetos a formatos serializables para JSON\"\"\"\n",
    "    if isinstance(obj, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (pd.DataFrame,)):\n",
    "        return obj.to_dict()\n",
    "    elif isinstance(obj, (dict,)):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "try:\n",
    "    with open('models/results_summary.json', 'w') as f:\n",
    "        json.dump(convert_to_serializable(results_summary), f, indent=2)\n",
    "    print(\"‚úÖ Resumen guardado en formato JSON\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error al guardar JSON: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Modelos y resultados guardados exitosamente:\")\n",
    "print(\"   üìÅ Todos los archivos guardados en la carpeta 'models/'\")\n",
    "print(\"   üîí Modelos binarios: binary_*.pkl\")\n",
    "print(\"   üéØ Modelos multiclase: multi_*.pkl\")\n",
    "print(\"   ‚öôÔ∏è  Preprocesador: feature_preprocessor.pkl\")\n",
    "print(\"   üè∑Ô∏è  Label encoder: label_encoder.pkl\")\n",
    "print(\"   üìä Resumen: results_summary.pkl y results_summary.json\")\n",
    "print(\"   üìà Dataset: github_commits_vulnerability_dataset.csv\")\n",
    "\n",
    "# Mostrar resumen de lo guardado\n",
    "print(f\"\\nüìã RESUMEN DE MODELOS GUARDADOS:\")\n",
    "print(f\"   Modelos binarios: {len(binary_results)}\")\n",
    "print(f\"   Modelos multiclase: {len(multi_results)}\")\n",
    "print(f\"   Mejor modelo binario: {results_summary.get('best_models', {}).get('binary', 'N/A')}\")\n",
    "print(f\"   Mejor modelo multiclase: {results_summary.get('best_models', {}).get('multiclass', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322bfcf9-aa45-4eb8-9e29-cc3ec8bbbe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICACI√ìN DE ARCHIVOS GUARDADOS\n",
    "print(\"üîç VERIFICANDO ARCHIVOS GUARDADOS...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import glob\n",
    "\n",
    "def verify_saved_files():\n",
    "    \"\"\"Verifica que todos los archivos se hayan guardado correctamente\"\"\"\n",
    "    expected_files = [\n",
    "        'models/github_commits_vulnerability_dataset.csv',\n",
    "        'models/feature_preprocessor.pkl',\n",
    "        'models/label_encoder.pkl',\n",
    "        'models/results_summary.pkl',\n",
    "        'models/results_summary.json'\n",
    "    ]\n",
    "    \n",
    "    # A√±adir modelos binarios\n",
    "    for name in binary_results.keys():\n",
    "        expected_files.append(f'models/binary_{name.lower().replace(\" \", \"_\")}.pkl')\n",
    "    \n",
    "    # A√±adir modelos multiclase\n",
    "    for name in multi_results.keys():\n",
    "        expected_files.append(f'models/multi_{name.lower().replace(\" \", \"_\")}.pkl')\n",
    "    \n",
    "    print(\"üìã Archivos esperados:\")\n",
    "    missing_files = []\n",
    "    existing_files = []\n",
    "    \n",
    "    for file_path in expected_files:\n",
    "        if os.path.exists(file_path):\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            existing_files.append((file_path, file_size))\n",
    "            print(f\"   ‚úÖ {file_path} ({file_size} bytes)\")\n",
    "        else:\n",
    "            missing_files.append(file_path)\n",
    "            print(f\"   ‚ùå {file_path} - NO ENCONTRADO\")\n",
    "    \n",
    "    print(f\"\\nüìä ESTAD√çSTICAS:\")\n",
    "    print(f\"   Total de archivos esperados: {len(expected_files)}\")\n",
    "    print(f\"   Archivos encontrados: {len(existing_files)}\")\n",
    "    print(f\"   Archivos faltantes: {len(missing_files)}\")\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"\\n‚ö†Ô∏è ARCHIVOS FALTANTES:\")\n",
    "        for missing in missing_files:\n",
    "            print(f\"   - {missing}\")\n",
    "    \n",
    "    return existing_files, missing_files\n",
    "\n",
    "# Ejecutar verificaci√≥n\n",
    "existing_files, missing_files = verify_saved_files()\n",
    "\n",
    "if not missing_files:\n",
    "    print(\"\\nüéâ ¬°Todos los archivos se guardaron correctamente!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Hay {len(missing_files)} archivos faltantes. Considera regenerarlos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10527792-15db-48bf-9b84-a73190747366",
   "metadata": {},
   "source": [
    "#### Funci√≥n para Cargar y Usar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d882d-8d54-4b93-84bd-8d50151af536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGA Y VERIFICACI√ìN DE MODELOS GUARDADOS\n",
    "print(\"üîÑ CARGANDO Y VERIFICANDO MODELOS GUARDADOS...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def load_and_test_models():\n",
    "    \"\"\"Carga los modelos guardados y realiza pruebas b√°sicas\"\"\"\n",
    "    try:\n",
    "        # Cargar resumen\n",
    "        results_summary = joblib.load('models/results_summary.pkl')\n",
    "        print(\"‚úÖ Resumen de resultados cargado\")\n",
    "        \n",
    "        # Cargar preprocesador\n",
    "        preprocessor_loaded = joblib.load('models/feature_preprocessor.pkl')\n",
    "        print(\"‚úÖ Preprocesador cargado\")\n",
    "        \n",
    "        # Cargar label encoder\n",
    "        label_encoder_loaded = joblib.load('models/label_encoder.pkl')\n",
    "        print(\"‚úÖ Label encoder cargado\")\n",
    "        \n",
    "        # Cargar mejor modelo binario\n",
    "        best_binary_name = results_summary['best_models']['binary']\n",
    "        binary_model_path = f'models/binary_{best_binary_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "        binary_model_loaded = joblib.load(binary_model_path)\n",
    "        print(f\"‚úÖ Mejor modelo binario cargado: {best_binary_name}\")\n",
    "        \n",
    "        # Cargar mejor modelo multiclase\n",
    "        best_multi_name = results_summary['best_models']['multiclass']\n",
    "        multi_model_path = f'models/multi_{best_multi_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "        multi_data_loaded = joblib.load(multi_model_path)\n",
    "        multi_model_loaded = multi_data_loaded['model']\n",
    "        multi_label_encoder = multi_data_loaded['label_encoder']\n",
    "        print(f\"‚úÖ Mejor modelo multiclase cargado: {best_multi_name}\")\n",
    "        \n",
    "        # Probar con ejemplo de commit\n",
    "        example_commit = {\n",
    "            'language': 'Python',\n",
    "            'lines_added': 45,\n",
    "            'lines_deleted': 12,\n",
    "            'files_changed': 2,\n",
    "            'cyclomatic_complexity': 18,\n",
    "            'has_input_validation': 0,\n",
    "            'has_escape_functions': 0,\n",
    "            'has_raw_queries': 1,\n",
    "            'has_string_concatenation': 1,\n",
    "            'has_inner_html': 0,\n",
    "            'developer_experience': 1.5,\n",
    "            'previous_vulnerabilities': 2,\n",
    "            'change_size': 57,\n",
    "            'risk_factor': 0.4\n",
    "        }\n",
    "        \n",
    "        # Realizar predicci√≥n de prueba\n",
    "        prediction = predict_commit_security(\n",
    "            binary_model_loaded,\n",
    "            multi_model_loaded,\n",
    "            multi_label_encoder,\n",
    "            example_commit\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüéØ PREDICCI√ìN DE PRUEBA:\")\n",
    "        print(f\"   Es vulnerable: {prediction['is_vulnerable']}\")\n",
    "        print(f\"   Tipo: {prediction['vulnerability_type']}\")\n",
    "        print(f\"   Confianza: {prediction['vulnerability_type_confidence']:.2%}\")\n",
    "        print(f\"   Nivel de riesgo: {prediction['risk_level']}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al cargar modelos: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar prueba de carga\n",
    "load_success = load_and_test_models()\n",
    "\n",
    "if load_success:\n",
    "    print(\"\\nüéâ ¬°Todos los modelos se cargaron y probaron exitosamente!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Hubo problemas al cargar los modelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1066c-8c6d-4590-99b6-abe01a2c542e",
   "metadata": {},
   "source": [
    "***Estructura de Entrada para Predicciones***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e6799-6005-413e-a853-7034c456965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_commit = {\n",
    "    'language': 'Python',  # JavaScript, Java, C++, etc.\n",
    "    'lines_added': 50,\n",
    "    'lines_deleted': 10,\n",
    "    'files_changed': 2,\n",
    "    'cyclomatic_complexity': 15,\n",
    "    'has_input_validation': 1,  # 1 = S√≠, 0 = No\n",
    "    'has_escape_functions': 1,\n",
    "    'has_raw_queries': 0,\n",
    "    'has_string_concatenation': 0,\n",
    "    'has_inner_html': 0,\n",
    "    'developer_experience': 2.5,\n",
    "    'previous_vulnerabilities': 1,\n",
    "    'change_size': 60,\n",
    "    'risk_factor': 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317f6f3-e818-42b8-bf24-cfc3d8fe1b63",
   "metadata": {},
   "source": [
    "# RESUMEN FINAL DEL PROYECTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a8778-117d-458f-b301-d9a4233f7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESUMEN FINAL DEL PROYECTO\n",
    "print(\"üéâ PROYECTO COMPLETADO EXITOSAMENTE!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def print_project_summary():\n",
    "    \"\"\"Imprime un resumen ejecutivo del proyecto\"\"\"\n",
    "    \n",
    "    # Cargar resumen si est√° disponible\n",
    "    try:\n",
    "        results_summary = joblib.load('models/results_summary.pkl')\n",
    "        \n",
    "        print(\"\\nüìä RESUMEN EJECUTIVO:\")\n",
    "        print(f\"üìç Dataset: {results_summary['dataset_info']['shape'][0]:,} commits\")\n",
    "        print(f\"üîí Proporci√≥n de vulnerabilidades: {results_summary['dataset_info']['vulnerable_ratio']:.2%}\")\n",
    "        print(f\"üåç Lenguajes analizados: {results_summary['dataset_info']['languages']}\")\n",
    "        print(f\"üéØ Tipos de vulnerabilidad: {results_summary['dataset_info']['vulnerability_types']}\")\n",
    "        \n",
    "        print(f\"\\nüèÜ MEJORES MODELOS:\")\n",
    "        best_models = results_summary.get('best_models', {})\n",
    "        if best_models:\n",
    "            print(f\"üîí Clasificaci√≥n Binaria: {best_models.get('binary', 'N/A')} - {best_models.get('binary_accuracy', 0):.2%}\")\n",
    "            print(f\"üéØ Clasificaci√≥n Multiclase: {best_models.get('multiclass', 'N/A')} - {best_models.get('multiclass_accuracy', 0):.2%}\")\n",
    "        \n",
    "        print(f\"\\nüìà RENDIMIENTO DE MODELOS BINARIOS:\")\n",
    "        for model, accuracy in results_summary['binary_results'].items():\n",
    "            print(f\"   {model}: {accuracy:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è No se pudo cargar el resumen: {e}\")\n",
    "        # Mostrar informaci√≥n b√°sica\n",
    "        print(f\"\\nüìä DATASET:\")\n",
    "        print(f\"   Filas: {df.shape[0]:,}\")\n",
    "        print(f\"   Columnas: {df.shape[1]}\")\n",
    "        print(f\"   Vulnerabilidades: {df['is_vulnerable'].mean():.2%}\")\n",
    "\n",
    "    print(f\"\\nüíæ ARCHIVOS GENERADOS:\")\n",
    "    model_files = glob.glob('models/*.pkl') + glob.glob('models/*.csv') + glob.glob('models/*.json')\n",
    "    for file_path in model_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print(f\"   {file_name} ({file_size} bytes)\")\n",
    "\n",
    "    print(f\"\\nüöÄ PR√ìXIMOS PASOS RECOMENDADOS:\")\n",
    "    print(\"   1. Integrar con APIs reales de GitHub\")\n",
    "    print(\"   2. Implementar en pipeline CI/CD\")\n",
    "    print(\"   3. Expandir a m√°s tipos de vulnerabilidades\")\n",
    "    print(\"   4. Recopilar dataset con commits reales\")\n",
    "    print(\"   5. Implementar sistema de alertas tempranas\")\n",
    "\n",
    "    print(f\"\\nüéØ USO EN PRODUCCI√ìN:\")\n",
    "    print(\"   from models import load_and_predict\")\n",
    "    print(\"   resultado = load_and_predict(nuevo_commit)\")\n",
    "    print(\"   if resultado['risk_level'] == 'ALTO':\")\n",
    "    print(\"       print('üö® Revisi√≥n de seguridad requerida')\")\n",
    "\n",
    "# Ejecutar resumen\n",
    "print_project_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3e722-8169-4988-b72e-16963b01319c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
